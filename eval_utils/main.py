# # coding=utf-8
# # Copyright (c) Meta Platforms, Inc. and affiliates.
# # All rights reserved.
# #
# # This source code is licensed under the license found in the
# # LICENSE file in the root directory of this source tree.

# # This code is based on QuaRot(https://github.com/spcl/QuaRot/tree/main/quarot).
# # Licensed under Apache License 2.0.


import os
import json # Ensure json is imported
import torch
import transformers

from eval_utils import gptq_utils, rotation_utils
from utils import data_utils, fuse_norm_utils, hadamard_utils, quant_utils, utils
from utils.convert_to_executorch import (
    sanitize_checkpoint_from_spinquant,
    write_model_llama,
)


def ptq_model(args, model, model_args=None): # args is ptq_args from process_args
    """
    Applies Post-Training Quantization (PTQ) modifications to the model,
    including optional rotation and Hadamard transforms based on arguments.
    """
    transformers.set_seed(args.seed)
    model.eval()

    # --- Determine Hadamard Mode based on args ---
    # These flags should be mutually exclusive (enforced by shell script or arg parser)
    is_global_had_mode = getattr(args, 'hadamard_online', False)
    is_selective_had_mode = args.selective_had_layers_path is not None
    is_r3_only_mode = args.online_r3_only is True #getattr(args, 'online_r3_only', False)
    # --- End Mode Determination ---

    print(f"INFO: global_had_mode: {is_global_had_mode}")
    print(f"INFO: selective_had_mode: {is_selective_had_mode}")
    print(f"INFO: r3_only_mode: {is_r3_only_mode}")

    # --- Load Selective Layer List (if applicable) ---
    selective_had_layers = None # For R4 selectivity
    if is_selective_had_mode:
        print(f"INFO: Selective Hadamard mode active. Loading layers from: {args.selective_had_layers_path}")
        if os.path.exists(args.selective_had_layers_path):
            try:
                with open(args.selective_had_layers_path, 'r') as f:
                    data = json.load(f)
                # Use the key generated by the analysis script ('layers_to_rotate')
                if "layers_to_rotate" in data and isinstance(data["layers_to_rotate"], list):
                    selective_had_layers = set(data["layers_to_rotate"])
                    print(f"INFO: Loaded {len(selective_had_layers)} layer indices for selective R4.")
                else:
                    print(f"Warning: JSON file found, but 'layers_to_rotate' key missing or not a list in {args.selective_had_layers_path}. R4 logic may default.")
            except Exception as e:
                print(f"Warning: Error loading/parsing JSON {args.selective_had_layers_path}: {e}. R4 logic may default.")
        else:
            print(f"Warning: Selective layer file not found: {args.selective_had_layers_path}. R4 logic may default.")
            is_selective_had_mode = False # Treat as non-selective if file not found
            selective_had_layers = None
    # --- End List Loading ---


    # --- Base Rotation (R1, R2 offline) ---
    if args.rotate:
        print("INFO: Applying base rotation (R1, R2 offline)...")
        fuse_norm_utils.fuse_layer_norms(model)
        # Pass args down so rotate_mlp_output can check flags/list for R4^T weight comp
        rotation_utils.rotate_model(model, args) # This calls modified rotate_mlp_output
        utils.cleanup_memory(verbos=True)
        quant_utils.add_actquant(model) # Add wrappers regardless of online had mode

        # --- Setup R4 (online_full_had for down_proj activations) ---
        # R4 is applied only in global or selective had mode, NOT in r3_only or no_had mode
        apply_r4_logic = is_global_had_mode or is_selective_had_mode
        print(f"INFO: Configuring R4 (down_proj online Hadamard). Mode active: {apply_r4_logic}")

        qlayers_r4 = quant_utils.find_qlayers(model)
        applied_r4_selectively_count = 0

        for name in qlayers_r4:
            if "down_proj" in name:
                # Default R4 to OFF unless conditions are met
                enable_r4_for_layer = False
                if apply_r4_logic: # Only proceed if global or selective mode is on
                    try:
                        layer_idx = int(name.split('.')[2])
                        if is_global_had_mode:
                            enable_r4_for_layer = True # Global mode: R4 ON
                        elif is_selective_had_mode and selective_had_layers is not None and layer_idx in selective_had_layers:
                            enable_r4_for_layer = True # Selective mode: R4 ON if in list
                            applied_r4_selectively_count += 1
                        # If selective mode but layer not in list, enable_r4_for_layer remains False
                    except (IndexError, ValueError):
                        print(f"Warning: Could not parse layer index from name '{name}'. R4 setting skipped.")

                # Apply config
                if hasattr(qlayers_r4[name], 'online_full_had'):
                    if enable_r4_for_layer:
                         qlayers_r4[name].online_full_had = enable_r4_for_layer
                         # Set other params only if enabling
                         had_K, K = hadamard_utils.get_hadK(model.config.intermediate_size)
                         qlayers_r4[name].had_K = had_K
                         qlayers_r4[name].K = K
                         qlayers_r4[name].fp32_had = args.fp32_had
                    # else: # Optional: Clear params if disabling? Depends on wrapper init
                    #    layer_module.had_K = None; layer_module.K = None
                else:
                    print(f"Warning: Layer {name} does not have 'online_full_had' attribute.")

        # --- Logging summary for R4 ---
        if is_selective_had_mode: print(f"INFO: Applied R4 selectively to {applied_r4_selectively_count} down_proj layers.")
        elif is_global_had_mode: print("INFO: Applied R4 globally to all down_proj layers.")
        else: print("INFO: R4 disabled (R3 only or No Had mode).")
        # --- End R4 Setup ---

    else: # No offline rotation at all (args.rotate is False)
        print("INFO: Skipping base rotation (R1, R2).")
        quant_utils.add_actquant(model) # Still add wrappers for standard quantization


    # --- Weight Quantization ---
    if args.w_bits < 16:
        print(f"INFO: Applying {args.w_bits}-bit weight quantization...")
        save_dict = {}
        if args.load_qmodel_path: # Load Quantized Rotated Model
            assert args.rotate, "Model should be rotated to load a quantized model!"
            assert (not args.save_qmodel_path), "Cannot save a quantized model if it is already loaded!"
            print(f"INFO: Loading pre-quantized model from {args.load_qmodel_path}")
            save_dict = torch.load(args.load_qmodel_path, map_location='cpu') # Load to CPU first
            model.load_state_dict(save_dict["model"])
            print("INFO: Pre-quantized model loaded.")
            # Note: If loading a pre-quantized model, subsequent GPTQ/RTN is skipped.
            # Ensure the loaded model matches the desired R3/R4 config.

        elif not args.w_rtn: # GPTQ Weight Quantization
            print("INFO: Using GPTQ for weight quantization.")
            trainloader = data_utils.get_wikitext2(
                nsamples=args.nsamples, seed=args.seed,
                model=model_args.input_model, seqlen=2048,
                eval_mode=False,
            )
            if args.export_to_et:
                print("INFO: Applying RTN to embeddings/head for Executorch export.")
                quantizers_et = gptq_utils.rtn_fwrd(
                    model, "cuda", args,
                    custom_layers=[model.model.embed_tokens, model.lm_head],
                )
            print("INFO: Running main GPTQ quantization...")
            quantizers = gptq_utils.gptq_fwrd(model, trainloader, "cuda", args)
            save_dict["w_quantizers"] = quantizers # Store quantizers if needed later
            print("INFO: GPTQ finished.")
        else: # RTN Weight Quantization
            print("INFO: Using RTN for weight quantization.")
            quantizers = gptq_utils.rtn_fwrd(model, "cuda", args)
            save_dict["w_quantizers"] = quantizers
            print("INFO: RTN finished.")

        if args.save_qmodel_path and not args.load_qmodel_path: # Save only if not loaded
            print(f"INFO: Saving quantized model state to {args.save_qmodel_path}")
            save_dict["model"] = model.state_dict()
            if args.export_to_et:
                print("INFO: Exporting model to Executorch format...")
                save_dict = write_model_llama(
                    model.state_dict(), model.config, num_shards=1
                )[0] # Export num_shards == 1 for executorch
                save_dict = sanitize_checkpoint_from_spinquant(
                    save_dict, group_size=args.w_groupsize
                )
            torch.save(save_dict, args.save_qmodel_path)
            print("INFO: Model saved.")


    # --- Activation/Input Quantization Configuration ---
    if args.a_bits < 16 or args.v_bits < 16:
        print(f"INFO: Configuring {args.a_bits}-bit activation / {args.v_bits}-bit V-cache quantization...")
        # Find layers with ActQuantWrapper (should have been added earlier)
        qlayers_act = quant_utils.find_qlayers(model, layers=[quant_utils.ActQuantWrapper])
        down_proj_groupsize = -1
        if args.a_groupsize > 0:
            down_proj_groupsize = utils.llama_down_proj_groupsize(model, args.a_groupsize)

        config = model.config # Get config for head dims etc.
        num_heads = config.num_attention_heads
        model_dim = config.hidden_size
        head_dim = model_dim // num_heads

        for name in qlayers_act:
            # Default settings for this layer
            layer_input_bits = args.a_bits
            layer_groupsize = args.a_groupsize
            layer_a_sym = not (args.a_asym)
            layer_a_clip = args.a_clip_ratio

            # Specific overrides based on layer type
            if "v_proj" in name and args.v_bits < 16: # Set the v_proj precision
                v_groupsize = head_dim # Group by head dim for V cache
                if hasattr(qlayers_act[name], 'out_quantizer'): # Check if out_quantizer exists
                     qlayers_act[name].out_quantizer.configure(
                         bits=args.v_bits, groupsize=v_groupsize,
                         sym=not (args.v_asym), clip_ratio=args.v_clip_ratio,
                     )
                else: print(f"Warning: {name} expected out_quantizer for V-cache but not found.")

            if "o_proj" in name:
                layer_groupsize = head_dim # Group by head dim for attention output input

            if "lm_head" in name: # Skip lm_head input quantization
                print(f"INFO: Skipping lm_head quantization for {name}.")
                layer_input_bits = 16 #layer_input_bits = 16

            if "down_proj" in name: # Set the down_proj input precision
                if args.int8_down_proj: 
                    layer_input_bits = 8
                layer_groupsize = down_proj_groupsize # Use calculated group size

            # Configure the main activation quantizer for the layer's input
            if hasattr(qlayers_act[name], 'quantizer'):
                 qlayers_act[name].quantizer.configure(
                     bits=layer_input_bits, groupsize=layer_groupsize,
                     sym=layer_a_sym, clip_ratio=layer_a_clip,
                 )
            else: print(f"Warning: {name} expected quantizer but not found.")
        print("INFO: Activation/V-cache quantization configured.")


    # --- Setup R3 (QKRotationWrapper for K-cache) ---
    # Apply R3 if k_bits < 16 AND (global had OR selective had OR r3_only mode is active)
    apply_r3_wrapper = (args.k_bits < 16) and (is_global_had_mode or is_selective_had_mode or is_r3_only_mode)

    if apply_r3_wrapper:
        print(f"INFO: Applying R3 QK Rotation Wrappers (k_bits={args.k_bits} and an online Had mode active).")
        if args.k_pre_rope: raise NotImplementedError("Pre-RoPE quantization is not supported yet!")
        else:
            rope_function_name = "apply_rotary_pos_emb"
            layers = model.model.layers
            k_quant_config = {
                "k_bits": args.k_bits,
                "k_groupsize": args.k_groupsize,
                "k_sym": not (args.k_asym),
                "k_clip_ratio": args.k_clip_ratio,
            }
            for layer_idx, layer in enumerate(layers):
                 if is_selective_had_mode:
                     if layer_idx in selective_had_layers:
                         # Selective mode: Apply R3 only to selected layers
                         rotation_utils.add_qk_rotation_wrapper_after_function_call_in_forward(
                             layer.self_attn, rope_function_name, config=model.config, **k_quant_config,
                         )
                     else:
                        print(f"Warning: R3 not applied to layer {layer_idx} (not in selective list).")
                 elif is_global_had_mode:
                     # Global mode: Apply R3 to all layers
                     rotation_utils.add_qk_rotation_wrapper_after_function_call_in_forward(
                         layer.self_attn, rope_function_name, config=model.config, **k_quant_config,
                     )
                 elif is_r3_only_mode:
                    rotation_utils.add_qk_rotation_wrapper_after_function_call_in_forward(
                        layer.self_attn, rope_function_name, config=model.config, **k_quant_config,
                    )
                 else:
                    print(f"Warning: R3 not applied to layer {layer_idx} (not in selective list).")
            print("INFO: R3 QK wrappers applied.")
    elif args.k_bits < 16:
        # k_bits < 16 but no online had mode active -> R3 skipped
        print(f"INFO: Skipping R3 QK Rotation Wrappers (k_bits={args.k_bits}, but no online Had mode selected).")
    # --- End R3 Setup ---

    print("INFO: ptq_model function finished.")
    return model